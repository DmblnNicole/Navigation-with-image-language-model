{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install segments-ai\n",
    "from segments import SegmentsClient, SegmentsDataset\n",
    "from segments.utils import get_semantic_bitmap\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "# Initialize a SegmentsDataset from the release file\n",
    "client = SegmentsClient('06bcb58a22ed6e6b10f075fc2bf8016ffcfda3b6')\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint='checkpoints/sam_vit_h_4b8939.pth').to(device='cuda')\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[[1,2],[1,2]],[[3,4],[3,4]],[[5,6],[5,6]]])\n",
    "A.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitmap2image(semantic_bitmap) -> Image:\n",
    "    A = np.asarray(semantic_bitmap, dtype=np.uint8)\n",
    "    image = np.zeros((A.shape[0], A.shape[1], 3), dtype=np.uint8)\n",
    "    image[:,:,0] = A*255\n",
    "    image[:,:,1] = A*255\n",
    "    image[:,:,2] = A*255\n",
    "    image = Image.fromarray(image, 'RGB')\n",
    "    return image\n",
    "\n",
    "def image2bitmap(image : Image, dtype=np.uint8):\n",
    "    image = np.asarray(image, dtype=dtype)\n",
    "    bitmap = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "    bitmap = image[:,:,0]\n",
    "    return bitmap\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    return mask_image\n",
    "\n",
    "def combine(image : Image, semantic_mask : Image) -> Image:\n",
    "    fig = Figure()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = fig.gca()\n",
    "    ax.imshow(image)\n",
    "    mask = show_mask(image2bitmap(semantic_mask, dtype=np.bool_), plt)\n",
    "    ax.imshow(mask)\n",
    "    ax.axis('off')\n",
    "    canvas.draw()\n",
    "    width, height = fig.get_size_inches() * fig.get_dpi() \n",
    "    img = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8).reshape(int(height), int(width), 3)\n",
    "    img = Image.fromarray(img, 'RGB')\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def get_bitmap_from_filename(filename: str):\n",
    "    resize = transforms.Resize((512,512))\n",
    "    a = Image.open(filename)\n",
    "    gt_mask = image2bitmap(resize(a))\n",
    "    return gt_mask\n",
    "\n",
    "def get_image_from_filename(filename: str):\n",
    "    resize = transforms.Resize((512,512))\n",
    "    a = Image.open(filename)\n",
    "    return np.asarray(resize(a))\n",
    "\n",
    "files = [\n",
    "    './processed_hike/mask_obstacle/wide_angle_camera_front_1677756696_625191528.png',\n",
    "    './processed_hike/mask_obstacle/wide_angle_camera_front_1677756688_627165488.png',\n",
    "    './processed_hike/mask_obstacle/wide_angle_camera_front_1677757468_681367022.png'\n",
    "]\n",
    "\n",
    "obstacle_masks = [get_bitmap_from_filename(file) for file in files]\n",
    "images = [get_image_from_filename(file) for file in files]\n",
    "\n",
    "zero_slice = np.argwhere(obstacle_masks[0] == 0)\n",
    "positive_slice = np.argwhere(obstacle_masks[0] == 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 61.33 GiB (GPU 0; 23.69 GiB total capacity; 2.97 GiB already allocated; 14.63 GiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m predictor\u001b[39m.\u001b[39mset_image(images[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m masks, _, _ \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m      3\u001b[0m             point_coords\u001b[39m=\u001b[39;49mpositive_slice,\n\u001b[1;32m      4\u001b[0m             point_labels\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mones(\u001b[39mlen\u001b[39;49m(positive_slice)),\n\u001b[1;32m      5\u001b[0m             multimask_output\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      6\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/predictor.py:154\u001b[0m, in \u001b[0;36mSamPredictor.predict\u001b[0;34m(self, point_coords, point_labels, box, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    151\u001b[0m     mask_input_torch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mask_input, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    152\u001b[0m     mask_input_torch \u001b[39m=\u001b[39m mask_input_torch[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m--> 154\u001b[0m masks, iou_predictions, low_res_masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_torch(\n\u001b[1;32m    155\u001b[0m     coords_torch,\n\u001b[1;32m    156\u001b[0m     labels_torch,\n\u001b[1;32m    157\u001b[0m     box_torch,\n\u001b[1;32m    158\u001b[0m     mask_input_torch,\n\u001b[1;32m    159\u001b[0m     multimask_output,\n\u001b[1;32m    160\u001b[0m     return_logits\u001b[39m=\u001b[39;49mreturn_logits,\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m masks \u001b[39m=\u001b[39m masks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    164\u001b[0m iou_predictions \u001b[39m=\u001b[39m iou_predictions[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/predictor.py:229\u001b[0m, in \u001b[0;36mSamPredictor.predict_torch\u001b[0;34m(self, point_coords, point_labels, boxes, mask_input, multimask_output, return_logits)\u001b[0m\n\u001b[1;32m    222\u001b[0m sparse_embeddings, dense_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mprompt_encoder(\n\u001b[1;32m    223\u001b[0m     points\u001b[39m=\u001b[39mpoints,\n\u001b[1;32m    224\u001b[0m     boxes\u001b[39m=\u001b[39mboxes,\n\u001b[1;32m    225\u001b[0m     masks\u001b[39m=\u001b[39mmask_input,\n\u001b[1;32m    226\u001b[0m )\n\u001b[1;32m    228\u001b[0m \u001b[39m# Predict masks\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m low_res_masks, iou_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mmask_decoder(\n\u001b[1;32m    230\u001b[0m     image_embeddings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures,\n\u001b[1;32m    231\u001b[0m     image_pe\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mprompt_encoder\u001b[39m.\u001b[39;49mget_dense_pe(),\n\u001b[1;32m    232\u001b[0m     sparse_prompt_embeddings\u001b[39m=\u001b[39;49msparse_embeddings,\n\u001b[1;32m    233\u001b[0m     dense_prompt_embeddings\u001b[39m=\u001b[39;49mdense_embeddings,\n\u001b[1;32m    234\u001b[0m     multimask_output\u001b[39m=\u001b[39;49mmultimask_output,\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    237\u001b[0m \u001b[39m# Upscale the masks to the original image resolution\u001b[39;00m\n\u001b[1;32m    238\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpostprocess_masks(low_res_masks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_size)\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/modeling/mask_decoder.py:94\u001b[0m, in \u001b[0;36mMaskDecoder.forward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     73\u001b[0m     image_embeddings: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m     multimask_output: \u001b[39mbool\u001b[39m,\n\u001b[1;32m     78\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m     79\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m    Predict masks given image and prompt embeddings.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m      torch.Tensor: batched predictions of mask quality\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     masks, iou_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_masks(\n\u001b[1;32m     95\u001b[0m         image_embeddings\u001b[39m=\u001b[39;49mimage_embeddings,\n\u001b[1;32m     96\u001b[0m         image_pe\u001b[39m=\u001b[39;49mimage_pe,\n\u001b[1;32m     97\u001b[0m         sparse_prompt_embeddings\u001b[39m=\u001b[39;49msparse_prompt_embeddings,\n\u001b[1;32m     98\u001b[0m         dense_prompt_embeddings\u001b[39m=\u001b[39;49mdense_prompt_embeddings,\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Select the correct mask or masks for outptu\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m multimask_output:\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/modeling/mask_decoder.py:132\u001b[0m, in \u001b[0;36mMaskDecoder.predict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    129\u001b[0m b, c, h, w \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mshape\n\u001b[1;32m    131\u001b[0m \u001b[39m# Run the transformer\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m hs, src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(src, pos_src, tokens)\n\u001b[1;32m    133\u001b[0m iou_token_out \u001b[39m=\u001b[39m hs[:, \u001b[39m0\u001b[39m, :]\n\u001b[1;32m    134\u001b[0m mask_tokens_out \u001b[39m=\u001b[39m hs[:, \u001b[39m1\u001b[39m : (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mask_tokens), :]\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/modeling/transformer.py:92\u001b[0m, in \u001b[0;36mTwoWayTransformer.forward\u001b[0;34m(self, image_embedding, image_pe, point_embedding)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m# Apply transformer blocks and final layernorm\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 92\u001b[0m     queries, keys \u001b[39m=\u001b[39m layer(\n\u001b[1;32m     93\u001b[0m         queries\u001b[39m=\u001b[39;49mqueries,\n\u001b[1;32m     94\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m     95\u001b[0m         query_pe\u001b[39m=\u001b[39;49mpoint_embedding,\n\u001b[1;32m     96\u001b[0m         key_pe\u001b[39m=\u001b[39;49mimage_pe,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39m# Apply the final attenion layer from the points to the image\u001b[39;00m\n\u001b[1;32m    100\u001b[0m q \u001b[39m=\u001b[39m queries \u001b[39m+\u001b[39m point_embedding\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/modeling/transformer.py:156\u001b[0m, in \u001b[0;36mTwoWayAttentionBlock.forward\u001b[0;34m(self, queries, keys, query_pe, key_pe)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m, queries: Tensor, keys: Tensor, query_pe: Tensor, key_pe: Tensor\n\u001b[1;32m    153\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    154\u001b[0m     \u001b[39m# Self attention block\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_first_layer_pe:\n\u001b[0;32m--> 156\u001b[0m         queries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(q\u001b[39m=\u001b[39;49mqueries, k\u001b[39m=\u001b[39;49mqueries, v\u001b[39m=\u001b[39;49mqueries)\n\u001b[1;32m    157\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         q \u001b[39m=\u001b[39m queries \u001b[39m+\u001b[39m query_pe\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/plr/lib/python3.9/site-packages/segment_anything/modeling/transformer.py:231\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m# Attention\u001b[39;00m\n\u001b[1;32m    230\u001b[0m _, _, _, c_per_head \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 231\u001b[0m attn \u001b[39m=\u001b[39m q \u001b[39m@\u001b[39;49m k\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m)  \u001b[39m# B x N_heads x N_tokens x N_tokens\u001b[39;00m\n\u001b[1;32m    232\u001b[0m attn \u001b[39m=\u001b[39m attn \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(c_per_head)\n\u001b[1;32m    233\u001b[0m attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(attn, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 61.33 GiB (GPU 0; 23.69 GiB total capacity; 2.97 GiB already allocated; 14.63 GiB free; 3.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "predictor.set_image(images[0])\n",
    "masks, _, _ = predictor.predict(\n",
    "            point_coords=positive_slice,\n",
    "            point_labels=np.ones(len(positive_slice)),\n",
    "            multimask_output=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216786"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ground truth for dirt paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Preloading all samples. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 82/82 [00:00<00:00, 325.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 82 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:32<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# release2 = client.get_release('mcummins/Hike2', 'v0.2') \n",
    "# dataset2 = SegmentsDataset(release2, labelset='ground-truth', filter_by=['reviewed'])\n",
    "# road_dir = []\n",
    "\n",
    "# for sample in tqdm(dataset2):\n",
    "#     semantic_bitmap = get_semantic_bitmap(sample['segmentation_bitmap'], sample['annotations'])\n",
    "#     dir = 'GT_masks/' + sample['name']\n",
    "#     road_dir.append(sample['name'])\n",
    "#     mask = bitmap2image(semantic_bitmap)\n",
    "#     mask.save(dir, format='png')\n",
    "#     illustration = combine(sample['image'], mask)\n",
    "#     illustration.save('GT/'+sample['name'], format='png')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ground truth for forest floors and concrete roads using SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset...\n",
      "Preloading all samples. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;255;153;0m██████████\u001b[0m| 205/205 [00:11<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 205 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [01:40<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# gt = os.listdir('GT_masks')\n",
    "\n",
    "# release = client.get_release('mcummins/Hike', 'V0.5') \n",
    "# dataset = SegmentsDataset(release, labelset='ground-truth', filter_by=['reviewed'])\n",
    "\n",
    "# for sample in tqdm(dataset):\n",
    "    \n",
    "#     if sample['name'] not in road_dir: \n",
    "\n",
    "#         points = []\n",
    "#         name = sample['name']\n",
    "#         image = sample['image']\n",
    "#         for ann in sample['annotations']:\n",
    "#             points.append(ann['points'])\n",
    "#         points = np.array(points)\n",
    "#         points = points.reshape((points.shape[0], points.shape[2]))\n",
    "\n",
    "#         predictor.set_image(np.asarray(image))\n",
    "#         input_point = points\n",
    "#         input_label = np.ones(points.shape[0])\n",
    "\n",
    "#         masks, _, _ = predictor.predict(\n",
    "#             point_coords=input_point,\n",
    "#             point_labels=input_label,\n",
    "#             multimask_output=False,\n",
    "#         )\n",
    "        \n",
    "#         mask = bitmap2image(masks[0])\n",
    "#         dir = 'GT_masks/' + sample['name']\n",
    "#         mask.save(dir, format='png')\n",
    "#         illustration = combine(image, mask)\n",
    "#         illustration.save('GT/'+sample['name'], format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
